# 100 Days Of Code - Log

### Day 1: 21 March 2018

**Today's Progress**: Enrolled in Andrew Ng's [Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning) course on Coursera, watched the first two introductory videos, and answered my first grading question. Also, watched five videos on vectors at [Khan Academy](https://www.khanacademy.org).

**Thoughts:** Everything I learned about vectors in school came flooding back (which is a credit to my maths teachers, as that was almost 20 years ago). There were some interesting things I didn't know before as well, like how to write the definition of a line in n dimensions. The machine learning introduction was mostly stuff that I knew before, although I expect that will quickly change. Andrew Ng's example of a neural network was very clear, and the easiest to understand I've seen yet.

### Day 2: 22 March 2018

**Today's Progress**: I've finished week one of the Neural Networks and Deep Learning course, including taking the quiz. That was a quick week! I got 10/10 on the quiz, yay. :) And I did some more brushing up on vectors on Khan Academy.

**Thoughts:** I still need more work on my vectors and matrices, so I think I will be doing them in parallel to the neural nets course. Week two of the course is going to involve a lot more of them, I'm guessing, so I'll have to judge my pace so that I learn enough linear algebra that I can progress, but not so much that I spend all of my time doing that.

### Day 3: 23 March 2018

**Today's Progress**: Spent all of my time today learning about matrices on Khan Academy.

**Thoughts:** It turns out that after 18 years you forget how to do matrix multiplication. I still have a way to go to get back up to speed, but hopefully I can mostly do that tomorrow.

### Day 4: 24 March 2018

**Today's Progress**: Also spent all of my time today learning about matrices on Khan Academy. This is becoming a theme. I covered 2D matrix transformations, and matrix inverses and determinants.

**Thoughts:** I was blown away by learning about matrix transformations, and realising that they basically underpin all of 3D computer graphics. When I get as far as matrix transposes I should be able to understand what's going on in the week 2 machine learning videos, so I'll switch back to Andrew Ng's course at that point.

### Day 5: 25 March 2018

**Today's Progress**: Finished studying matrices and vectors at Khan Academy (for now), and am starting back on the machine learning course videos.

**Thoughts:** I now know a lot more about matrices than I did a few days ago, and the logistic regression algorithm in Andrew Ng's course is starting to make sense. I feel I'm making good progress.

### Day 6: 26 March 2018

**Today's Progress**: Watched all of the compulsory week 2 videos for the Neural Networks and Deep Learning course.

**Thoughts:** Today's videos were mostly about Python, which I'm a lot more at home with than the maths. Numpy is still new for me though, so it's still not exactly plain sailing. Tomorrow I'll be going on to the programming assignment. Finally, I'll be writing real code!

### Day 7: 27 March 2018

**Today's Progress**: Started working through the introduction to numpy in the Neural Networks and Deep Learning course (hereby abbreviated to NNDL).

**Thoughts:** There were a few new concepts in the introduction - notably vector and matrix norms - so it was slow going. It was also my first time working with Jupyter notebooks, which are awesome so far (and I hear they get better). I can tell that it's going to take some time before I'm comfortable with all the maths here, but hey, that's the point, right? :)

### Day 8: 28 March 2018

**Today's Progress**: Did the week two quiz in the NNDL course, and worked on the numpy introduction some more.

**Thoughts:** I only got 9/10 on the quiz due to a careless mistake (boo). Otherwise, it wasn't too bad. The maths is still the bottleneck for me, but it's coming.

### Day 9: 29 March 2018

**Today's Progress**: Finished the numpy introduction, and started on the first required programming assignment.

**Thoughts:** The result of the programming assignment will be a real machine learning algorithm, so I'm pretty excited. :) Still, it's slow going, as I'm still getting used to numpy and matrices.

### Day 10: 30 March 2018

**Today's Progress**: Just studied more about vectors today.

**Thoughts:** No earth-shattering discoveries today, but I did get to learn some interesting properties of vector dot products.

### Day 11: 31 March 2018

**Today's Progress**: Continued the week two programming assignment.

**Thoughts:** My first successful forward propagation and back-propagation. Nice! Those cat pics aren't quite classified yet, though.

### Day 12: 1 April 2018

**Today's Progress**: Finished the week two programming assignment. I now have a bona fide cat recognition algorithm. At the moment it has 70% accuracy.

**Thoughts:** Finishing week two feels good! I didn't find the programming hard - it was just learning the new ML concepts and getting used to NumPy, really. I actually found the "Introduction to NumPy" programming assignment trickier than the graded assignment.

### Day 13: 2 April 2018

**Today's Progress**: Started on the week 3 videos, and also started brushing up on my calculus at Khan Academy. (Today I went over the delta/epsilon limit proof, which I vaguely remember from school, but not well enough to feel comfortable going further in calculus without.) 

**Thoughts:** Again, I'm going to have to pace myself with calculus - I don't want to study it to the exclusion of the ML course and then have to do my assignments in a rush. But I have a strong feeling that a good understanding of it will help me. And hopefully the calculus from school will start coming back once I get back into it a bit.

### Day 14: 3 April 2018

**Today's Progress**: Watched videos about differential calculus on Khan Academy.

**Thoughts:** Just going over the basics again. In my school maths class we never learned limits formally, only informally, but I think that differential calculus is much easier to understand when you understand the formal definition of limits first.

### Day 15: 4 April 2018

**Today's Progress**: Watched more videos about differential calculus on Khan Academy.

**Thoughts:** Mmm, nutritious maths. Today I got past the formal introduction to the derivative of a function, and got on to the power rule. This is much more familiar to me - I remember doing a lot of this stuff in school. I think I'll go as far as the chain rule and then go back to the machine learning course.

### Day 16: 5 April 2018

**Today's Progress**: Studied the chain rule, the product rule, and the quotient rule on Khan Academy, then tried to find the derivative of the sigmoid function.

**Thoughts:** I bought off a bit more than I could chew with the sigmoid function! I failed when I tried by myself, but I now know/remember enough calculus to understand the [derivation on Stack Exchange](https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x).

### Day 17: 6 April 2018

**Today's Progress**: Finished off the videos for week 3 of NNDL, found the derivative of the tanh function, and passed the week 3 quiz.

**Thoughts:** I was impressed with myself for differentiating tanh without any help, although I did know what the answer was supposed to be. I (again) only got 9 out of 10 on the quiz though, (again) due to a careless mistake. But overall, a good day's study.

### Day 18: 7 April 2018

**Today's Progress**: Spent today going back through the videos for week three and the quizzes for week 2 and week 3.

**Thoughts:** I spent quite a lot of time going through the dimensions of the different matrices in the different layers of the neural networks, and now I think I have a much better handle of what is going on. And because in NNDL you can take the quizzes as many times as you want and they keep your top score, I now have 10/10 on both. Yay!

### Day 19: 8 April 2018

**Today's Progress**: Started the programming exercise for week 3 of NNDL.

**Thoughts:** I haven't quite finished the exercise, although I did finish the model - my first neural network with a hidden layer.

### Day 20: 9 April 2018

**Today's Progress**: Finished week 3 of NNDL, and started on the videos for week 4.

**Thoughts:** On a similar theme as week two, I didn't find the programming exercise that difficult this time. In fact, it was easier this time, as it was so similar to the week two exercise. It's quite fun to see how even a neural network with just one hidden layer can do a much better job at fitting data than logistic regression can. At this rate, I'll be able to finish week four in a few days.

### Day 21: 10 April 2018

**Today's Progress**: Finished the videos and the quiz for week 4 of NNDL.

**Thoughts:** I got 10/10 on the quiz. (Finally!) And I feel I have a good understanding of the material now, which is probably why.

### Day 22: 11 April 2018

**Today's Progress**: Finished the first programming exercise for week 4 of NNDL.

**Thoughts:** It was tough starting today as I got back pretty late from work, but I still managed to finish my study at a not-too-unreasonable time. The programming exercise was basically the same as for week three, but generalised to any number of layers of network instead of just two. Tomorrow, hopefully I will be correctly classifying some cute little kitties.

### Day 23: 12 April 2018

**Today's Progress**: Finished the course! And I had a bit of time left over, so I watched the video about the Caucy-Schwartz inequality on Khan Academy.

**Thoughts:** The last programming exercise was the easiest of the lot - it was basically just copying and pasting function definitions that I'd written before. The result was pretty good though - my cat-recognition model is now up to 80% accuracy.

### Day 24: 13 April 2018

**Today's Progress**: Started on Andrew Ng's Machine Learning course at Coursera, and took the first quiz.

**Thoughts:** This course seems a lot easier than NNDL - it looks like I took them the wrong way round - but there are lots of details in here that I think will help cement my understanding of neural networks, and give me some broader machine learning knowledge. I can easily imagine missing a useful approach to a problem if all I know about is neural nets. The course is supposed to take 11 weeks, but I imagine I'll finish it much quicker than that.

### Day 25: 14 April 2018

**Today's Progress**: Learned more about series on Khan Academy.

**Thoughts:** The first thing in the machine learning course is linear regression, and it looks like to really understand it I will need to be able to differentiate a series, which means I need to properly understand series. (Serieses? Nope, that's not right.) So I spent a lot of time muddling through arithmetic series today, and I was surprised at how little I remembered about them from school. They make a lot more sense after seeing Sal's proof for the general formula of an arithmetic series on Khan Academy. The actual differentiation part will come later.

### Day 26: 15 April 2018

**Today's Progress**: Learned more about linear regression in the ML course, more series on Khan Academy, and also started looking at multivariable functions on Khan Academy.

**Thoughts:** It turns out that in week one of the ML course they solve linear regression using gradient descent, although it can apparently be solved directly as well. It makes sense that you have to use gradient descent for neural networks, as the input space is so complex, but I'm impressed to learn that there are direct methods for simpler ML models as well. I'm going to need to learn more of the maths to understand all of this properly. Good job I have a whole 74 days left. ;)

### Day 27: 16 April 2018

**Today's Progress**: Finished week 1 of the ML course, and started on the week 2 videos. Finished all of the linear regression ones already.

**Thoughts:** I was pleased to see that I could do the week 1 linear algebra quiz without looking at any of the Coursera linear algebra videos - obviously what I learned from those Khan Academy videos has stuck.

### Day 28: 17 April 2018

**Today's Progress**: Learned more about partial derivatives on Khan Academy, then rewatched Patrick Winston's neural net lecture for the MIT Artificial Intelligence course.

**Thoughts:** Watching the lecture the second time made a lot more sense than watching it the first time! I think the first time was a little more than a year ago, and I had no idea about partial derivatives then. Still, I can tell that I still have quite a bit of work to do to be fully mathematically literate at this stuff.

### Day 29: 18 April 2018

**Today's Progress**: Watched week two videos for the Coursera ML course, did the linear regression quiz, and started playing around with Octave.

**Thoughts:** I only got 3/5 on the quiz on my first attempt. Both of the ones I got wrong were silly mistakes (forgot a minus sign, and forgot a parameter). But that's not the end of the world - I got all of them right on the second attempt. Octave seems fun so far - I have a feeling I'm going to enjoy the programming exercises this time. Apparently you can do them on Matlab too, but why use that when you can GNU?

### Day 30: 19 April 2018

**Today's Progress**: Watched the rest of the Octave videos on Coursera and took the Octave quiz.

**Thoughts:** Octave has a few surprises, but nothing extremely new. I do like the "magic" function - it's very useful for trying stuff out. Next up is the programming exercise, but scheduling means I'll probably have to stick with videos instead. Either Khan Academy or part two of Patrick Winston's neural nets lecture, I think.

### Day 31: 20 April 2018

**Today's Progress**: Watched videos about partial differentiation and parametric functions on Khan Academy. 

**Thoughts:** I ended up going with the Khan Academy videos today. Lots of good stuff there, and I think I have a good understanding of partial differentiation now, although I could probably do with more practice at actually solving some of them.

### Day 32: 21 April 2018

**Today's Progress**: Started the linear regression programming exercise for the ML Coursera course.

**Thoughts:** I now have linear regression with one variable working as it should be. I am finding Octave harder going than the Python used in the NNDL course - which makes sense, as I use Python every day, but Octave is new for me - but at least I won't be totally out of my depth now if someone asks me to program something in MATLAB. Tomorrow I'll finish off the optional part fo the programming assignment (linear regression with multiple variables), and then I'll move on to week three and new ML models. 

### Day 33: 22 April 2018

**Today's Progress**: Finished the linear regression programming exercise and started watching the week three videos.

**Thoughts:** Week three is about logistic regression, which I have already covered in NNDL, but the ML course seems more in-depth, so I think I'll probably learn some new things. From a couple of things in the videos so far, it looks like I'll need to brush up on my statistics as well.

### Day 34: 23 April 2018

**Today's Progress**: Finished the videos and quizzes for week three.

**Thoughts:** Perhaps there were less videos for week three than for week two, but I seemed to get through them very quickly this time. Just the programming exercise left, and I'll be on week four.

### Day 35: 24 April 2018

**Today's Progress**: Started the programming exercise for week three.

**Thoughts:** A lot of today's time was spent working out how to plot nice graphs in Octave, and a lot of the rest was spent on realising that I had forgotten to plug my sigmoid function into my logistic regression model. But I am steadily getting more comfortable with Octave, which is good. Hopefully tomorrow I'll be able to finish the programming exercise off.

### Day 36: 25 April 2018

**Today's Progress**: Finished the programming exercise for week three.

**Thoughts:** And with that, week three is over. It's been interesting to learn about regularisation, and about the alternatives to gradient descent, and implementing them was surprisingly easy. (Well, I didn't implement the gradient descent alternative myself, to be fair - I just called an Octave function.) Tomorrow it's onto week four and neural networks.

### Day 37: 26 April 2018

**Today's Progress**: Finished the videos and the quiz for week four.

**Thoughts:** Week four is about neural network basics, so this is familiar ground for me. Although Prof. Ng uses slightly different notation than he does for the NNDL course, which is confusing.

### Day 38: 27 April 2018

**Today's Progress**: Finished the programming exercise for week four.

**Thoughts:** Similarly to yesterday, the programming exercise was all familiar stuff. In fact, apparently Prof. Ng thought that people would implement logistic regression using a for loop for the week three assignment, and this week's challenge was to implement it in vectorised form. However, my solution from last week three was already vectorised, so it was literally just copy and paste to pass that part.

### Day 39: 28 April 2018

**Today's Progress**: Finished the videos and the quiz for week five.

**Thoughts:** These weeks are starting to go by very quickly! Well, this one was about back-propogation in neural networks, which is another thing I've covered before. Next week is about advice for applying machine learning, which will probably be mostly new, and then the week after is about support vector machines, which is definitely new. So I'll have to see if I can keep up this pace.

### Day 40: 29 April 2018

**Today's Progress**: Started the programming exercise for week five.

**Thoughts:** I spent most of today staring blankly at the cost function for neural networks using multiple classification, and wondering how to implement it in Octave in a vectorised way. I understand the basic idea, but I can't seem to get it to work. Will try again tomorrow...

### Day 41: 30 April 2018

**Today's Progress**: Still working through the programming exercise. I managed to do the bit I was stuck on yesterday, which was good.

**Thoughts:** This programming exercise is definitely harder than the previous ones. I was glad that I managed to figure out yesterday's problem, though. I'm definitely starting to get the hang of vectorised stuff, even if my progress is still a little slower than I would like.
